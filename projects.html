<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Publications and Projects</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>
<body class="flex flex-col min-h-screen bg-black text-white">
    <a href="index.html" class="text-white p-4 absolute top-4 left-4">
        <i class="fas fa-home fa-2x"></i>
    </a>
    <main class="flex flex-1 flex-col items-center justify-center p-4">
        <section id="projects" class="w-full max-w-4xl mx-auto bg-gray-900 border rounded-lg shadow-md p-4">
            <h1 class="text-2xl font-bold mb-4">Publications and Projects</h1>
            <div class="grid grid-cols-1 gap-6">
                <div class="box border rounded-lg shadow-md p-4">
                    <h2 class="text-xl font-semibold mb-2">Publications (In Progress)</h2>
                    <ul class="list-disc pl-5 mb-4">
                        <li>
                            <strong>Self-Improving Instructions and Programs for Visual Concept Learning using LLMs at a Human Level</strong><br>
                            <span style="color: #ccc;"><b><i>Ayush Sharma</i></b>, Yuke Zhang, Madeleine Udell, Iddo Drori, In progress</span>
                        </li>
                        <li class="mt-2">
                            <strong>Solving Harvard's Mathematics PhD Quals and MIT's EECS Curriculum using LLMs at a Human Level</strong><br>
                            <span style="color: #ccc;">Iddo Drori, Danielle Drori, Cindy Zhang, Ryan Nie, Chunhao Bi, <b><i>Ayush Sharma</i></b>, Uday Garg, Shreyas Sudarsan, Seunghwan Hyun, Bargav Jagatha, Akshat Gurbaxani, Abhaya Shukla, Nicholas Belsten, Ori Kerret, Avi Shporer, Madeleine Udell, In progress</span>
                        </li>
                    </ul>
                </div>
                <!-- Project 1 -->
<div class="box border rounded-lg shadow-md p-4">
    <img src="ai-research-assistant.png" alt="Research Assistant" class="w-full h-40 object-cover object-top mb-4">
    <h2 class="text-xl font-semibold mb-2">AI Research Paper and arXiv Assistant</h2>
    <p class="mb-4"><b>Tech Used:</b> Retrieval-augmented generation (RAG), LangChain, Google Gemini 1.5 Flash LLM, MLOps, Meta FAISS, Streamlit</p>
    <p class="mb-4"><b>Description of the project:</b></p>
    <ul class="list-disc pl-5 mb-4">
        <li>An End to End Deployed AI Application!</li>
        <li>This assistant allows users to upload multiple research papers (PDF format) or provide ArXiv links, enabling them to query the content and receive accurate, context-aware responses. </li>
        <li>This assistant uses advanced document retrieval techniques combined with the power of Google Gemini 1.5 Flash LLM and LangChain to generate detailed, insightful answers to user questions. It leverages Retrieval-Augmented Generation (RAG) to retrieve relevant context from the uploaded documents and provides intelligent, context-aware responses. I implemented the Meta's FAISS for document similarity search.</li>
        <li>The frontend allows user to input either mulitple PDFs or multiple arXiv links, then ask their query and do research to get responses. It is created and deployed using Streamlit.</li>
    </ul>
    <div class="flex justify-between">
        <a href="https://your-ai-research-assistant.streamlit.app/" target="_blank" class="text-yellow-300 hover:underline">Website Link</a>
    </div>
</div>
                <!-- Project 2 -->
                <div class="box border rounded-lg shadow-md p-4">
                    <img src="diffusion.png" alt="Diffusion" class="w-full h-40 object-cover mb-4">
                    <h2 class="text-xl font-semibold mb-2">Photo-realistic Video Generation using Diffusion Models</h2>
                    <p class="mb-4"><b>Tech Used:</b> Text to Image Diffusion Models, DDIM and DPM Samplers, Image Generation, Generative AI</p>
                    <p class="mb-4"><b>Description of the project:</b></p>
                    <ul class="list-disc pl-5 mb-4">
                        <li>A research project which aims to improve the state-of-the-art zero-shot text-guided video-to-video framework 'Rerender A Video'.</li>
                        <li>We implemented modifications to the original paper’s methods for key frame sampling and frame selection.</li>
                        <li>While the original authors used Uniform Key Frame Selection, we implemented Non-Uniform Key Frame Selection using various methods such as ORB Feature Points Matching, Optical Flow Change Calculation, and ResNet-Based Feature Extraction.</li>
                        <li>Additionally, we used the DPM solver as a sampler, unlike the original authors who used the DDIM sampler.</li>
                    </ul>
                    <div class="flex justify-between">
                        <a href="https://github.com/ChunhBi/Rerender_A_Video" target="_blank" class="text-yellow-300 hover:underline">GitHub Repo</a>
                        <a href="Photorealistic Video Generation.pdf" target="_blank" class="text-yellow-300 hover:underline">Project Research Report</a>
                    </div>
                </div>
                <!-- project 3 -->
                <div class="box border rounded-lg shadow-md p-4">
                    <img src="wgbh.png" alt="WGBH" class="w-full h-40 object-cover mb-4">
                    <h2 class="text-xl font-semibold mb-2">Debt Collection project for client "WGBH" (Team Lead)</h2>
                    <p class="mb-4"><b>Tech Used:</b> Data Science and Analysis, SQL, Python, Pandas, Matplotlib</p>
                    <p class="mb-4"><b>Description of the project:</b></p>
                    <ul class="list-disc pl-5 mb-4">
                        <li>A Data Science Project, led by me, for the Public Radio and Television Station of Boston, “WGBH,” in Collaboration with Boston University.</li>
                        <li>We examined 5,000+ debt collection cases (from Massachusetts Court System Database) over 10 years for the client "WGBH",
                            identifying a 20% rise in debt case filings and a 25% increase in virtual proceedings during pandemic (2020-2021), and
                            other important insights like 40% capias warrants, 30% wage garnishments, top 10 debt collectors and companies, etc.</li>
                        <li>I led a team of 4 students in this project.</li>
                    </ul>
                    <div class="flex justify-between">
                        <a href="https://github.com/iamcalledayush/Debt-collection" target="_blank" class="text-yellow-300 hover:underline">GitHub Repo</a>
                        <a href="WGBH.pdf" target="_blank" class="text-yellow-300 hover:underline">Project Results</a>
                    </div>
                </div>
                <!-- project 4 -->
                <div class="box border rounded-lg shadow-md p-4">
                    <img src="GAN.png" alt="GANs" class="w-full h-40 object-cover mb-4">
                    <h2 class="text-xl font-semibold mb-2">Image Generation using Generative Adversarial Networks</h2>
                    <p class="mb-4"><b>Tech Used:</b> Generative AI, Generative Adversarial Networks, BigGANs, PyTorch</p>
                    <p class="mb-4"><b>Description of the project:</b></p>
                    <ul class="list-disc pl-5 mb-4">
                        <li>This research project aims to explore various types of GANs.</li>
                        <li>I Developed various models like basic GAN and BigGAN to generate dog images using the standard Stanford Dog Dataset.</li>
                        <li>I employed Self Attention and other architectural changes in these models to overcome issues like Mode Collapse, Artifacts problem, etc.</li>
                    </ul>
                    <div class="flex justify-between">
                        <a href="https://github.com/iamcalledayush/Exploring-GANs" target="_blank" class="text-yellow-300 hover:underline">GitHub Repo</a>
                        <a href="GANs.pdf" target="_blank" class="text-yellow-300 hover:underline">Project Report</a>
                    </div>
                </div>

                <!-- project 5 -->
                <div class="box border rounded-lg shadow-md p-4">
                    <img src="car.png" alt="Car Detection System" class="w-full h-40 object-cover mb-4">
                    <h2 class="text-xl font-semibold mb-2">Autonomous-Driving Car Detection</h2>
                    <p class="mb-4"><b>Tech Used:</b> Object Detection and Tracking, Deep Convolutional Neural Networks, YOLO
                        algorithm, Non-max suppression & Intersection over Union</p>
                    <p class="mb-4"><b>Description of the project:</b></p>
                    <ul class="list-disc pl-5 mb-4">
                        <li>The goal of this project is to develop a robust object detection system that can identify various objects and vehicles in real-time from a car-mounted camera. This is a crucial component in building self-driving car software.</li> 
                        <li>For this purpose, I utilized a deep learning model based on the YOLO (You Only Look Once) algorithm. The dataset used for training and testing the model was provided by Drive.ai and consists of a diverse set of 80 object classes captured from Silicon Valley roadways.</li>
                        <li>Techniques like non-max suppression & Intersection over Union were used for anchor box reduction.</li>
                    </ul>
                    <div class="flex justify-between">
                        <a href="https://github.com/IAMCALLEDAYUSH/AUTONOMOUS-DRIVING-CAR-DETECTION" target="_blank" class="text-yellow-300 hover:underline">GitHub Repo</a>
                    </div>
                </div>

                <!-- project 46 -->
                <div class="box border rounded-lg shadow-md p-4">
                    <img src="recommendation.png" alt="Recommendation System" class="w-full h-40 object-cover mb-4">
                    <h2 class="text-xl font-semibold mb-2">Movie Recommendation System</h2>
                    <p class="mb-4"><b>Tech Used:</b>Collaborative Filtering, Python, Numpy, Pandas</p>
                    <p class="mb-4"><b>Description of the project:</b></p>
                    <ul class="list-disc pl-5 mb-4">
                        <li>In this project, a collaborative filtering-based recommendation system was developed to suggest movies to users based on their previous ratings.</li>
                        <li>The primary goal of collaborative filtering here is to learn two key components: user preference vectors and movie feature vectors. The dot product of these vectors, combined with a bias term, provides an estimated rating for a given movie by a user.</li>
                        <li>The project leverages the MovieLens dataset, which contains ratings from a range of users for different movies on a scale of 0.5 to 5.</li>
                    </ul>
                    <div class="flex justify-between">
                        <a href="https://github.com/iamcalledayush/Collaborative-Filtering---Recommendations" target="_blank" class="text-yellow-300 hover:underline">GitHub Repo</a>
                    </div>
                </div>                
            </div>
        </section>
    </main>

    <footer class="footer text-center p-4">
        <p>Email: <a href="https://mail.google.com/mail/?view=cm&fs=1&to=ayushsharmacorp@gmail.com" target="_blank" class="hover:text-yellow-300">ayushsharmacorp@gmail.com</a> | Phone: <a href="tel:+16173097097" class="hover:text-yellow-300">+1 (617) 309-7097</a></p>
    </footer>
</body>
</html>
